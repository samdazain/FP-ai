{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Initial Setup\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"tweets-data.csv\")\n",
    "\n",
    "# Display a message indicating that the data has been successfully loaded\n",
    "print(\"Loaded data:\")\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with a nicer table format\n",
    "display(data.head(5))\n",
    "\n",
    "# Indonesian stopwords from NLTK and additional stopwords\n",
    "stop_words_indo_nltk = stopwords.words('indonesian')\n",
    "stop_words_eng_nltk = stopwords.words('english')\n",
    "additional_stopwords = [\n",
    "  'yang', 'di', 'ke', 'dari', 'ini', 'itu', 'pada', 'untuk', 'dan', 'dengan',\n",
    "  'adalah', 'saya', 'kamu', 'dia', 'kita', 'mereka', 'akan', 'atau', 'seperti', \n",
    "  'FFFF00', 't co', 'FFFF00 ', 'https', 'segyongstar', 'ipi', 'ye', 'ha', 'a', 't', \n",
    "  'co' , 'i', 'font', 'fontcolor', 'fontcolor=', 'mkkkkkkkkkkk', '=', '#', '\"', 'FFFF00', 'ffff'\n",
    "]\n",
    "stop_words_id = list(set(stop_words_indo_nltk + stop_words_eng_nltk + additional_stopwords))\n",
    "\n",
    "# Initialize Sastrawi Stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Cleaning and Stemming\n",
    "\n",
    "# Select the 'full_text' column for sentiment analysis\n",
    "text = data[\"full_text\"]\n",
    "\n",
    "# Display a message indicating the selected text column\n",
    "print(\"Selected text column:\")\n",
    "\n",
    "# Set pandas display options to show all rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Convert the series to a DataFrame with the column named \"full_text\"\n",
    "text_df = pd.DataFrame(text, columns=[\"full_text\"])\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "display(text_df.head(10))\n",
    "\n",
    "# Function to clean and stem the text data\n",
    "def clean_and_stem_text(text):\n",
    "text = text.lower() # Lowercase the text\n",
    "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text) # Remove special characters\n",
    "text = re.sub(r'https?://\\S+', '', text)\n",
    "text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "text = text.replace('\\t', ' ').replace('\\n', ' ').replace('\\\\u', '\n",
    "').replace('\\\\', '') # Replace escape characters\n",
    "text = text.encode('ascii', 'replace').decode('ascii') # Encode to ASCII\n",
    "tokens = word_tokenize(text) # Tokenize the text\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words_id] #\n",
    "Remove stopwords\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens] # Stem the tokens\n",
    "return \" \".join(stemmed_tokens) # Join the cleaned and stemmed tokens\n",
    "                                                          \n",
    "# Clean and stem the text data\n",
    "text = text.apply(clean_and_stem_text)\n",
    "\n",
    "# Set pandas display options to show all rows and full width of columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Save the cleaned and stemmed text to a new DataFrame\n",
    "cleaned_data = pd.DataFrame({'cleaned_text': text})\n",
    "\n",
    "print(\"Cleaned and stemmed text data:\")\n",
    "display(cleaned_data.head(20))\n",
    "\n",
    "# Save the cleaned and stemmed text data to a CSV file\n",
    "cleaned_data.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Translation\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the cleaned data\n",
    "cleaned_data = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Load the MarianMT model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-id-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        # Tokenize the text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        # Generate translation\n",
    "        translated = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text\n",
    "\n",
    "# Translate the cleaned and stemmed text data\n",
    "cleaned_data['translated_text'] =\n",
    "cleaned_data['cleaned_text'].apply(translate_text)\n",
    "\n",
    "# Set pandas display options to show all rows and full width of columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Translated text data:\")\n",
    "display(cleaned_data['translated_text'].head(20))\n",
    "\n",
    "# Save the translated text to a new CSV file\n",
    "cleaned_data.to_csv(\"translated_data.csv\", index=False)\n",
    "print(\"Translated data saved to translated_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
