{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sastrawi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSastrawi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStemmer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStemmerFactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StemmerFactory\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the data from the CSV file\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Sastrawi'"
     ]
    }
   ],
   "source": [
    "# 1. Data Loading and Initial Setup\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"tweets-data.csv\")\n",
    "\n",
    "# Display a message indicating that the data has been successfully loaded\n",
    "print(\"Loaded data:\")\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with a nicer table format\n",
    "display(data.head(5))\n",
    "\n",
    "# Indonesian stopwords from NLTK and additional stopwords\n",
    "stop_words_indo_nltk = stopwords.words('indonesian')\n",
    "stop_words_eng_nltk = stopwords.words('english')\n",
    "additional_stopwords = [\n",
    "  'yang', 'di', 'ke', 'dari', 'ini', 'itu', 'pada', 'untuk', 'dan', 'dengan',\n",
    "  'adalah', 'saya', 'kamu', 'dia', 'kita', 'mereka', 'akan', 'atau', 'seperti', \n",
    "  'FFFF00', 't co', 'FFFF00 ', 'https', 'segyongstar', 'ipi', 'ye', 'ha', 'a', 't', \n",
    "  'co' , 'i', 'font', 'fontcolor', 'fontcolor=', 'mkkkkkkkkkkk', '=', '#', '\"', 'FFFF00', 'ffff'\n",
    "]\n",
    "stop_words_id = list(set(stop_words_indo_nltk + stop_words_eng_nltk + additional_stopwords))\n",
    "\n",
    "# Initialize Sastrawi Stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Cleaning and Stemming\n",
    "\n",
    "# Select the 'full_text' column for sentiment analysis\n",
    "text = data[\"full_text\"]\n",
    "\n",
    "# Display a message indicating the selected text column\n",
    "print(\"Selected text column:\")\n",
    "\n",
    "# Set pandas display options to show all rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Convert the series to a DataFrame with the column named \"full_text\"\n",
    "text_df = pd.DataFrame(text, columns=[\"full_text\"])\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "display(text_df.head(10))\n",
    "\n",
    "# Function to clean and stem the text data\n",
    "def clean_and_stem_text(text):\n",
    "text = text.lower() # Lowercase the text\n",
    "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text) # Remove special characters\n",
    "text = re.sub(r'https?://\\S+', '', text)\n",
    "text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "text = text.replace('\\t', ' ').replace('\\n', ' ').replace('\\\\u', '\n",
    "').replace('\\\\', '') # Replace escape characters\n",
    "text = text.encode('ascii', 'replace').decode('ascii') # Encode to ASCII\n",
    "tokens = word_tokenize(text) # Tokenize the text\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words_id] #\n",
    "Remove stopwords\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens] # Stem the tokens\n",
    "return \" \".join(stemmed_tokens) # Join the cleaned and stemmed tokens\n",
    "                                                          \n",
    "# Clean and stem the text data\n",
    "text = text.apply(clean_and_stem_text)\n",
    "\n",
    "# Set pandas display options to show all rows and full width of columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Save the cleaned and stemmed text to a new DataFrame\n",
    "cleaned_data = pd.DataFrame({'cleaned_text': text})\n",
    "\n",
    "print(\"Cleaned and stemmed text data:\")\n",
    "display(cleaned_data.head(20))\n",
    "\n",
    "# Save the cleaned and stemmed text data to a CSV file\n",
    "cleaned_data.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Translation\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the cleaned data\n",
    "cleaned_data = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Load the MarianMT model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-id-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        # Tokenize the text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        # Generate translation\n",
    "        translated = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text\n",
    "\n",
    "# Translate the cleaned and stemmed text data\n",
    "cleaned_data['translated_text'] =\n",
    "cleaned_data['cleaned_text'].apply(translate_text)\n",
    "\n",
    "# Set pandas display options to show all rows and full width of columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Translated text data:\")\n",
    "display(cleaned_data['translated_text'].head(20))\n",
    "\n",
    "# Save the translated text to a new CSV file\n",
    "cleaned_data.to_csv(\"translated_data.csv\", index=False)\n",
    "print(\"Translated data saved to translated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sentiment Analysis \n",
    " \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd \n",
    " \n",
    "# Create an instance of Vader SentimentIntensityAnalyzer \n",
    "analyzer = SentimentIntensityAnalyzer() \n",
    " \n",
    "# Load the translated data \n",
    "translated_data = pd.read_csv(\"translated_data.csv\") \n",
    " \n",
    "# Extract the translated text column \n",
    "translated_text = translated_data[\"translated_text\"] \n",
    " \n",
    "# Function to add sentiment score to the data \n",
    "def get_sentiment(translated_text): \n",
    "    if pd.isna(translated_text): \n",
    "        return None  # Handle NaN values \n",
    "    sentiment = analyzer.polarity_scores(translated_text) \n",
    "    return sentiment[\"compound\"] \n",
    " \n",
    "# Add sentiment score to the translated data \n",
    "translated_data[\"sentiment\"] = \n",
    "translated_data[\"translated_text\"].apply(get_sentiment) \n",
    " \n",
    "print(\"Translated data with sentiment score:\") \n",
    "display(translated_data[['translated_text', 'sentiment']].head(20)) \n",
    " \n",
    "# Save the data with sentiment scores to a new CSV file \n",
    "translated_data.to_csv(\"translated_data_with_sentiment.csv\", index=False) \n",
    "print(\"Translated data with sentiment scores saved to \n",
    "translated_data_with_sentiment.csv\") \n",
    " \n",
    "# Function to classify sentiment score into positive, neutral, or negative \n",
    "def sentiment_label(score): \n",
    "  if score > 0.05: \n",
    "    return \"positive\" \n",
    "  elif score < -0.05: \n",
    "    return \"negative\" \n",
    "  else: \n",
    "    return \"neutral\" \n",
    " \n",
    "# Load the translated data with sentiment scores \n",
    "sentiment_data = pd.read_csv(\"translated_data_with_sentiment.csv\") \n",
    " \n",
    "# Add sentiment label to the data \n",
    "sentiment_data[\"sentiment_label\"] = \n",
    "sentiment_data[\"sentiment\"].apply(sentiment_label) \n",
    " \n",
    "print(\"Data with sentiment label:\") \n",
    "# Create a DataFrame with 'translated_text', 'sentiment', and 'sentiment_label' \n",
    "columns \n",
    "selected_data = sentiment_data[['translated_text', 'sentiment', \n",
    "'sentiment_label']] \n",
    " \n",
    "# Set pandas display options to show the full width of the 'translated_text' \n",
    "column \n",
    "pd.set_option('display.max_colwidth', None) \n",
    " \n",
    "# Display the DataFrame in table format \n",
    "display(selected_data.head(20)) \n",
    " \n",
    "# Save the DataFrame with sentiment labels to a new CSV file \n",
    "selected_data.to_csv(\"sentiment_data_with_labels.csv\", index=False) \n",
    "print(\"Sentiment data with labels saved to sentiment_data_with_labels.csv\")"
]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
"# 5. Visualization \n",
" \n",
"import matplotlib.pyplot as plt \n",
"from wordcloud import WordCloud \n",
" \n"
"# Load the translated data with sentiment labels \n",
"final_data = pd.read_csv("sentiment_data_with_labels.csv") \n",
" \n"
"# Fill NaN values with empty strings and convert all entries to strings \n",
"final_data['translated_text'] = \n",
"final_data['translated_text'].fillna('').astype(str) \n",
" \n"
"# Combine all the translated text into a single string \n",
"text_combined = " ".join(final_data['translated_text']) \n",
" \n"
"# Create and configure the word cloud \n",
"wordcloud = WordCloud( \n",
"width=800, \n",
"height=400, \n",
"background_color='white', \n",
"stopwords=set(stop_words_indo_nltk + stop_words_eng_nltk + \n",
"additional_stopwords), \n",
"max_words=200, \n",
"contour_color='steelblue', \n",
"contour_width=3 \n",
").generate(text_combined) \n",
" \n"
"# Display the word cloud using matplotlib \n",
"plt.figure(figsize=(10, 5)) \n",
"plt.imshow(wordcloud, interpolation='bilinear') \n",
"plt.axis('off') \n",
"plt.title('Word Cloud of Data Tweets') \n",
"plt.show() \n",
" \n"
"# Count the sentiment labels \n",
"sentiment_counts = final_data["sentiment_label"].value_counts() \n",
" \n"
"# Create a table to show sentiment counts \n",
"sentiment_table = pd.DataFrame({"Sentiment": sentiment_counts.index, "Count":  \n",
"sentiment_counts.values}) \n",
"print(sentiment_table.to_string(index=False)) \n",
" \n"
"# Create a histogram to visualize sentiment distribution \n",
"plt.bar(sentiment_counts.index, sentiment_counts.values) \n",
"plt.xlabel("Sentiment") \n",
"plt.ylabel("Count") \n",
"plt.title("Sentiment Distribution of UKT Increase Comments") \n",
"plt.show() "
    ]
   } 
   ],
  

 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
